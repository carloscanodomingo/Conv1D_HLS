{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Saccade Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab qt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from numpy import ndarray, argmax, hstack\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "\n",
    "from eyelib.datasets import load\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "dataset = load('full_saccadic')\n",
    "X = dataset['X']\n",
    "Y = dataset['Y']\n",
    "\n",
    "def iterate_batches(x, y, batch_size) -> Tuple[ndarray, ndarray]:\n",
    "    index = 0\n",
    "    while index < len(x):\n",
    "        yield x[index:index+batch_size,:], y[index:index+batch_size,:]\n",
    "        index += batch_size\n",
    "    \n",
    "def train_test_validation_split(x: ndarray, y: ndarray, train_ratio: float=0.7, test_ratio: float=0.2) -> Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray]:\n",
    "    tmp = hstack((x, y))\n",
    "    shuffle(tmp)\n",
    "    first_split = int(len(tmp) * train_ratio)\n",
    "    second_split = first_split + int(len(tmp) * test_ratio)\n",
    "    train = tmp[:first_split,:]\n",
    "    test = tmp[first_split:second_split,:]\n",
    "    validation = tmp[second_split:,:]\n",
    "    return train[:,:x.shape[1]], train[:,x.shape[1]:], test[:,:x.shape[1]], test[:,x.shape[1]:], validation[:,:x.shape[1]], validation[:,x.shape[1]:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de la Relu (?, 1, 192, 16)valor de pool_width2\n",
      "Salida del maxpool (?, 1, 96, 16)valor de pool_width2\n",
      "(?, 1, 96, 16)\n",
      "Salida de la Relu (?, 1, 96, 32)valor de pool_width2\n",
      "Salida del maxpool (?, 1, 48, 32)valor de pool_width2\n",
      "pool_sizes_mul 4\n",
      "Epoch: 1 cost = 56.841024 test accuracy: 0.469537\n",
      "Epoch: 2 cost = 56.354034 test accuracy: 0.454102\n",
      "Epoch: 3 cost = 58.261151 test accuracy: 0.470349\n",
      "Epoch: 4 cost = 57.963040 test accuracy: 0.469537\n",
      "Epoch: 5 cost = 59.615530 test accuracy: 0.479285\n",
      "Epoch: 6 cost = 60.794984 test accuracy: 0.480097\n",
      "Epoch: 7 cost = 63.344562 test accuracy: 0.463851\n",
      "Epoch: 8 cost = 64.704462 test accuracy: 0.461413\n",
      "Epoch: 9 cost = 66.898454 test accuracy: 0.463851\n",
      "Epoch: 10 cost = 67.709950 test accuracy: 0.464663\n",
      "Epoch: 11 cost = 68.264326 test accuracy: 0.464663\n",
      "Epoch: 12 cost = 68.831947 test accuracy: 0.463038\n",
      "Epoch: 13 cost = 69.398125 test accuracy: 0.462226\n",
      "Epoch: 14 cost = 69.262723 test accuracy: 0.462226\n",
      "Epoch: 15 cost = 69.584415 test accuracy: 0.460601\n",
      "Epoch: 16 cost = 69.976638 test accuracy: 0.462226\n",
      "Epoch: 17 cost = 70.000986 test accuracy: 0.461413\n",
      "Epoch: 18 cost = 70.227467 test accuracy: 0.461413\n",
      "Epoch: 19 cost = 70.019416 test accuracy: 0.458976\n",
      "Epoch: 20 cost = 70.606089 test accuracy: 0.461413\n",
      "Epoch: 21 cost = 70.206508 test accuracy: 0.461413\n",
      "Epoch: 22 cost = 65.711917 test accuracy: 0.462226\n",
      "Epoch: 23 cost = 70.539628 test accuracy: 0.461413\n",
      "Epoch: 24 cost = 71.264722 test accuracy: 0.461413\n",
      "Epoch: 25 cost = 70.942798 test accuracy: 0.461413\n",
      "Epoch: 26 cost = 70.834935 test accuracy: 0.461413\n",
      "Epoch: 27 cost = 71.095892 test accuracy: 0.461413\n",
      "Epoch: 28 cost = 70.933947 test accuracy: 0.461413\n",
      "Epoch: 29 cost = 70.831785 test accuracy: 0.461413\n",
      "Epoch: 30 cost = 71.171107 test accuracy: 0.460601\n",
      "Epoch: 31 cost = 70.909037 test accuracy: 0.461413\n",
      "Epoch: 32 cost = 71.095819 test accuracy: 0.461413\n",
      "Epoch: 33 cost = 71.038427 test accuracy: 0.463038\n",
      "Epoch: 34 cost = 70.856932 test accuracy: 0.461413\n",
      "Epoch: 35 cost = 70.845509 test accuracy: 0.461413\n",
      "Epoch: 36 cost = 70.921724 test accuracy: 0.461413\n",
      "Epoch: 37 cost = 70.840125 test accuracy: 0.461413\n",
      "Epoch: 38 cost = 70.929855 test accuracy: 0.461413\n",
      "Epoch: 39 cost = 70.816747 test accuracy: 0.464663\n",
      "Epoch: 40 cost = 70.729052 test accuracy: 0.466288\n",
      "Epoch: 41 cost = 70.683719 test accuracy: 0.469537\n",
      "Epoch: 42 cost = 70.801839 test accuracy: 0.467100\n",
      "Epoch: 43 cost = 70.416021 test accuracy: 0.468725\n",
      "Epoch: 44 cost = 68.839866 test accuracy: 0.506905\n",
      "Epoch: 45 cost = 69.044346 test accuracy: 0.480097\n",
      "Epoch: 46 cost = 69.397722 test accuracy: 0.491470\n",
      "Epoch: 47 cost = 72.140922 test accuracy: 0.468725\n",
      "Epoch: 48 cost = 71.156392 test accuracy: 0.470349\n",
      "Epoch: 49 cost = 69.330054 test accuracy: 0.511779\n",
      "Epoch: 50 cost = 72.411552 test accuracy: 0.470349\n",
      "Epoch: 51 cost = 71.154606 test accuracy: 0.471162\n",
      "Epoch: 52 cost = 70.919510 test accuracy: 0.471974\n",
      "Epoch: 53 cost = 67.312306 test accuracy: 0.506093\n",
      "Epoch: 54 cost = 69.279368 test accuracy: 0.476848\n",
      "Epoch: 55 cost = 71.181052 test accuracy: 0.470349\n",
      "Epoch: 56 cost = 70.819988 test accuracy: 0.471162\n",
      "Epoch: 57 cost = 70.429408 test accuracy: 0.472786\n",
      "Epoch: 58 cost = 70.086068 test accuracy: 0.471974\n",
      "Epoch: 59 cost = 70.310011 test accuracy: 0.472786\n",
      "Epoch: 60 cost = 68.112368 test accuracy: 0.493095\n",
      "Epoch: 61 cost = 71.715826 test accuracy: 0.471162\n",
      "Epoch: 62 cost = 70.569753 test accuracy: 0.473599\n",
      "Epoch: 63 cost = 70.225920 test accuracy: 0.473599\n",
      "Epoch: 64 cost = 69.075550 test accuracy: 0.480097\n",
      "Epoch: 65 cost = 70.614834 test accuracy: 0.472786\n",
      "Epoch: 66 cost = 70.185432 test accuracy: 0.474411\n",
      "Epoch: 67 cost = 69.632073 test accuracy: 0.476036\n",
      "Epoch: 68 cost = 69.532094 test accuracy: 0.478473\n",
      "Epoch: 69 cost = 69.993517 test accuracy: 0.476848\n",
      "Epoch: 70 cost = 69.352690 test accuracy: 0.476848\n",
      "Epoch: 71 cost = 69.232227 test accuracy: 0.476036\n",
      "Epoch: 72 cost = 69.149819 test accuracy: 0.477660\n",
      "Epoch: 73 cost = 69.294649 test accuracy: 0.476848\n",
      "Epoch: 74 cost = 69.200811 test accuracy: 0.476848\n",
      "Epoch: 75 cost = 68.934943 test accuracy: 0.476036\n",
      "Epoch: 76 cost = 68.946530 test accuracy: 0.474411\n",
      "Epoch: 77 cost = 68.666655 test accuracy: 0.476848\n",
      "Epoch: 78 cost = 69.085948 test accuracy: 0.477660\n",
      "Epoch: 79 cost = 68.982418 test accuracy: 0.478473\n",
      "Epoch: 80 cost = 68.702900 test accuracy: 0.478473\n",
      "Epoch: 81 cost = 68.516364 test accuracy: 0.478473\n",
      "Epoch: 82 cost = 68.270453 test accuracy: 0.479285\n",
      "Epoch: 83 cost = 68.933241 test accuracy: 0.478473\n",
      "Epoch: 84 cost = 67.449508 test accuracy: 0.473599\n",
      "Epoch: 85 cost = 68.971753 test accuracy: 0.479285\n",
      "Epoch: 86 cost = 68.220489 test accuracy: 0.480097\n",
      "Epoch: 87 cost = 68.054728 test accuracy: 0.479285\n",
      "Epoch: 88 cost = 68.558363 test accuracy: 0.477660\n",
      "Epoch: 89 cost = 68.143821 test accuracy: 0.476036\n",
      "Epoch: 90 cost = 67.845500 test accuracy: 0.478473\n",
      "Epoch: 91 cost = 63.938883 test accuracy: 0.493907\n",
      "Epoch: 92 cost = 68.400705 test accuracy: 0.477660\n",
      "Epoch: 93 cost = 68.093599 test accuracy: 0.478473\n",
      "Epoch: 94 cost = 67.963850 test accuracy: 0.476848\n",
      "Epoch: 95 cost = 67.859630 test accuracy: 0.478473\n",
      "Epoch: 96 cost = 64.613448 test accuracy: 0.484159\n",
      "Epoch: 97 cost = 69.440698 test accuracy: 0.480910\n",
      "Epoch: 98 cost = 68.262261 test accuracy: 0.478473\n",
      "Epoch: 99 cost = 67.743370 test accuracy: 0.479285\n",
      "Epoch: 100 cost = 67.742909 test accuracy: 0.476848\n",
      "\n",
      "Training complete!\n",
      "0.39059967\n",
      "\n",
      "Confusion Matrix\n",
      "[[234   2   0]\n",
      " [237   6   0]\n",
      " [105  33   0]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00005\n",
    "epochs = 100\n",
    "batch_size = 25\n",
    "dropout = 0.9\n",
    "\n",
    "samples_count = 192\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, samples_count])\n",
    "y = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "x_shaped = tf.reshape(x, [-1, 1, samples_count, 1])\n",
    "\n",
    "def conv_layer(input_data, num_input_channels, num_filters, filter_width, pool_width, name):\n",
    "    conv_filt_shape = [1, filter_width, num_input_channels, num_filters]\n",
    "    \n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal(conv_filt_shape, stddev=0.03), \n",
    "        name=f'{name}_W'\n",
    "    )\n",
    "    bias = tf.Variable(\n",
    "        tf.truncated_normal([num_filters]), \n",
    "        name=f'{name}_b'\n",
    "    )\n",
    "    \n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME') + bias\n",
    "    \n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "    \n",
    "    ksize = [1, 1, pool_width, 1]\n",
    "    strides = [1, 1, pool_width, 1]\n",
    "    print(\"Salida de la Relu \" + str(out_layer.shape) + \"valor de pool_width\" + str(pool_width))\n",
    "    out_layer = tf.nn.max_pool(value=out_layer, ksize=ksize, strides=strides, padding='SAME')\n",
    "    print(\"Salida del maxpool \" + str(out_layer.shape)+ \"valor de pool_width\" + str(pool_width))\n",
    "    return out_layer\n",
    "\n",
    "conv1_pool_width = 2  # 192 -> 96\n",
    "conv2_pool_width = 2  # 96 -> 48\n",
    "\n",
    "conv1 = conv_layer(input_data=x_shaped, num_input_channels=1, num_filters=16, filter_width=9,pool_width= conv1_pool_width, name='conv1')\n",
    "print(conv1.shape)\n",
    "conv2 = conv_layer(conv1, 16, 32, 7, conv2_pool_width, name='conv2')\n",
    "\n",
    "pool_sizes_mul = conv1_pool_width * conv2_pool_width\n",
    "print(\"pool_sizes_mul \" + str(pool_sizes_mul))\n",
    "fully_connected_samples = int(samples_count / pool_sizes_mul) * 32\n",
    "\n",
    "flattened = tf.reshape(conv2, [-1, fully_connected_samples])\n",
    "\n",
    "\n",
    "wd1 = tf.Variable(tf.truncated_normal([fully_connected_samples, 768], stddev=0.03), name='wd1')\n",
    "bd1 = tf.Variable(tf.truncated_normal([768], stddev=0.01), name='bd1')\n",
    "layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "layer1 = tf.nn.relu(layer1)\n",
    "layer1 = tf.nn.dropout(layer1, dropout)\n",
    "\n",
    "wd2 = tf.Variable(tf.truncated_normal([768, 3], stddev=0.03), name='wd2')\n",
    "bd2 = tf.Variable(tf.truncated_normal([3], stddev=0.01), name='bd2')\n",
    "layer2 = tf.matmul(layer1, wd2) + bd2\n",
    "y_ = tf.nn.softmax(layer2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=layer2, labels=y))\n",
    "\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "cm = tf.confusion_matrix(tf.argmax(y, 1), tf.argmax(y_, 1), num_classes=3)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    train_x, train_y, test_x, test_y, validate_x, validate_y = train_test_validation_split(X, Y)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for batch_x, batch_y in iterate_batches(train_x, train_y, batch_size):\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / (len(batch_x) / batch_size)\n",
    "        \n",
    "        test_acc = sess.run(accuracy, feed_dict={x: test_x, y: test_y})\n",
    "        print(f\"Epoch: {epoch + 1} cost = {avg_cost:.6f} test accuracy: {test_acc:.6f}\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(sess.run(accuracy, feed_dict={x: validate_x, y: validate_y}))\n",
    "    \n",
    "    print('\\nConfusion Matrix')\n",
    "    print(sess.run(cm, feed_dict={x: validate_x, y: validate_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536, 768)\n",
      "(?, 3)\n"
     ]
    }
   ],
   "source": [
    "print(wd1.shape)\n",
    "print(y_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.shuffle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
